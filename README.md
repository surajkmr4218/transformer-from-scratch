# Transformer from Scratch

A minimal implementation of the Transformer architecture built from first principles and trained on text data.

## Overview

This repository contains a clean implementation of the attention-based Transformer model originally introduced in "Attention is All You Need" (Vaswani et al., 2017).

## Features

- Multi-head self-attention mechanism
- Positional encoding
- Encoder-decoder architecture
- Layer normalization and residual connections
- Training loop with standard optimization (Adam)
- Text tokenization and dataset handling
